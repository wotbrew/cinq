---

one pass aggregation perf

group-fusion works again, but it does not perform as well as I'd like.

the current impl binds an object-array with a slot for each acc variable and bangs on it.

---

one pass summarization only

we have two different aggregation engines. Hard to maintain. Realising arrays only works for a subset of cases. Consider
only supporting the eager one pass aggregation

e.g sql style, symbol refs to columns outside of an aggregate can return a (undefined) sample.

array_agg style function could be used if you want groups

in order to do this, I think we will need to be certain the planner rewrites for group-let work in every case.

---

Indexes do not use native filters, does this matter?

---

Identifiers

Variable = keyword, optionally qualified
Index = pair [?variable ?keyword]

---

Primary, Unique, FK

Before a complete picture is formed around schema, start with discrete operations that do the work on the db.

;; might be nice, to have a data literal for 'index key', including 'relvar' - this makes more sense when considering foreign keys.
(c/add-index db [:customers :birthdate])

;; index can then be looked up with the db using both components, or rel using second component
(get db [:customers :birthdate]) == (get-in db [:customers :birthdate])

;; associate :unique true with an existing index, throw if already a value on insert
;; throw if index does not exist
(c/add-unique-index db [:customers :email] true)

;; perhaps if indexes were created either as unique or not, rather than toggled - we can use a different type.
;; I think a unique index can behave like an eager clojure map of key -> val, rather than key -> rel (for all keys).
;; i.e (-> db :customers :email (get "foo@bar.com)) returns {:email "foo@bar.com", ...}"
;;     (-> db :customers :email (put "foo@bar.com" {:email "foo@bar.com", ...})) (assert on :email = key).
;; can then do put-if-absent, upd, upd-or-merge etc etc.

;; add a foreign key into the
(foreign-key db [:orders :customer-id] [:customers :id] {:on-delete :cascade})

- On insert test if value when given is a hit for the supplied index.
- When :customer is deleted, a cascades array is checked to :cascade or :set-nil the referenced order.
  Behaviour if not specified is to crash (as a constraint would be invalidated)

Like create-index, these functions create and return an index. In the case of foreign keys, it may only attach or modify metadata associated with an existing index (that it is a foreign key).

---

Spilling

spilled merge join
(q [c customers
    :join! [o orders (= o:customer-id c:customer-id]]
    [c o])

spilled group by
(q [c customers
    :group! [firstname c:firstname]]
    [firstname (c/count)])

spilled sorts
(q [c customers
    :order! [c:firstname :desc]]
    c:id)

Assumption that this should be explicit, is that correct?

Consider also non user space implicits, semi/anti join, apply join

Consider also limits, how big can the sort files get? How long can such queries take?
How do we interrupt them?

If we can fuse grouping, perhaps we could get away without this, if a user can tell what parts of the query
would be materialized, they could find another way around it.

---

CinqDynamicBigRecord
- used if map is big
- actual hash table with lazy value loads

---

CinqDynamicList
- perhaps for large lists, lazy decoding would be helpful

---

CinqDynamicSet
(same as maps)

---

Record specialisation (structs)

(c/defrecord Customer [a b c d e f])

adds an actual defrecord, with a type hint.
record name is mapped to a layout (array of name, type)

layouts are stored in the symbol table
records are stored as layout value* buffers

(q [^Customer c customers])
 provides a layout hint to (scan customers) so that decoder can be specialised

if layout hint not matched adapt to the record using map->X ?
Or perhaps emitted/hinted type should be an interface, which can have an adapter for a lazy record

---

Hardening

- min map size (maybe 128KB?)
- variable limit (LMDB)
- symbol limit (warn and degrade?)

---

Destructure numeric key

(q [[a b] foo] [a b])

vector destructures should leave a special (nth-lookup) form in the scan so that we choose nth over get or dispatch at runtime based
on whether associative?

---

Views / Rules

(c/defview ancestor [parent]
  parent
  (c/q [[a b] ancestor
        [c d] ancestor
        :when (= b c)]
        [a b]))

defines macro-like the ra
[:union parent $join]

Problems:
 - hygiene, do we need to gensym and unquote?
 - Perhaps all unresolvable globally unqualified symbols get renamed to gensym?

usage

(cte [parent2 [["bob" "phil"]]
      ancestor (ancestor parent2)]
   ancestor)

[:cte [[parent ...], [ancestor [:cte [[ancestor [:union parent $join]]]]]]]

rules:

flatten unions into same bind
flatten cte into cte

---

Non materialized CTE views

---

Mutual recursion

Not supported at the moment. How to make that fast?

Non recursivity carries to dependents, if a query/view is only dependent on non recursive queries it itself is non recursive
this would be a planner / ana change

---

Interrupted checks

Currently, if the loop gets out of control (CTE?) you are screwed as there are no interrupt checks.
This will likely be even more important later when you are scanning 2TB LMDB relations.

Putting interrupt checks naively on the main loop does come with a fairly significant perf cost or around 5%.

If reads were buffered this cost may go down? Or if we go down the columnar route we can put the check at vector points

---

Nil as a record

Yay or nay, I expect a few things might be trickier if nils are allowed (we'd need to box to disambiguate for e.g lmdb delete return)

---

LMDB map size

map resize lock, guarantee exclusivity!
ReadWrite lock, write lock used when map needs resizing (and possibly DDL?),
read lock used for lmdb transactions (read/write) consider fairness barging.
for now all transactions completely lock the database!

---

Test db per index

This could mean a some multiple of the number of tables as opened dbis and extra per transaction overhead. Test though, might be fine.
If we have a problem consider prefixing keys with a single byte (0 == table), (1+) == index. Can have a max of 254 indexes per table.

---

Escape analysis

If columns do not escape a query, we do not need to copy them into heap memory.

We are already doing this a little bit to choose an unsafe top-level type for records.

- if captured by a 'build' such as group/join hash table, we still can keep refs to native mem
- if used in an expression (or intermediate) expression that might escape (due to side effect or return from query, or embedding in a return)
  we need to copy

---

immu / bitemp

- immutable relvar
  implies system_from, system_to cols
  update = cut
  delete = cut

  implies $now denormalized index for each var. use c/latest to return a new db/tx which wraps all relvars
  implies $now index for each index (-> db c/latest :foo :id (get 42))

- bitemporal relvar

  implies system_from, system_to, valid_from, valid_to cols
  update = cut
  delete = cut

  implies $now denormalized index for each var. use c/latest to return a new db/tx which wraps all relvars
  implies $now index for each index (-> db c/latest :foo :id (get 42))

---

javascript

- queries emit a lot of code, what to do about that?
- indexeddb rather than lmdb
- async transactions / reads
- conditional compilation and specialisation based on prim types would not be present

---

Index intersection

(c/merge (c/get idx0 42) (c/range idx2 < 42))

Add a rsn scan interface method to indexes to do some int-intersection

---

Composite indexes

option varargs:
how to get the index you cannot do (:foo db).

option, vec:
(c/create-index foo [:a :b :c])
;; partial lookup
(c/getn (foo [:a :b :c]) :a 42 :b 43)

equality would allow you to do partial lookup, range, asc, top-n would have to only work on the prefix attribute

- lists / vectors should be ok?
- what does index-key look like
- how does partial index query work
- how are these encoded correctly

---

Hash only indexes

- indexing maps and sets will give you hash lookups but no range queries (obviously)
- prefix coding a hash (find algo that preserves unorderedness of structures)
- should sorted maps / sets get special treatment? i.e these could get a their own typeid?

---

Constraints

Unique (and primary?)
Foreign key
Schema

Encoded as data, saved in variables table

(c/create-relvar db :foo schema) supply initial schema

(c/set-schema relvar schema)
;; throw if cannot enforce

(c/drop-schema relvar)
;; wild west

upserts, insert-ignore etc possible with primary/unique

---

extended destructuring

current limitations

- nesting/composites, e.g [[a b]], {[x, y] :z}
- lookup syms as bare symbols in bind e.g (c/q [a:b coll] a:b)

Currently bind does not follow clj destructuring, as maps do not have a seq path for example

---

Throw an error if opening a write tx inside a read tx. read nesting, write nesting might be fine.

---

scan chunking

there is a lot of virtual call overhead calling .apply per-tuple.

we could permit buffering some number of tuples before calling .apply

- filtering can set bits in a bitmap
- apply can capture the loop with the reduced checks
- apply can take an array and a len parameter

in the case of native, you can .apply to an lmdb cursor, and specialise the loop, including binary filters.

---

prim maps for integer joins

joins could get specialised hash multimap for long keys. common case if we have auto-incr, sql data.

---

analyzer

shadowing columns could cause interesting problems potentially, particularly if we want to use group-by fusion
investigate whether we need to do anything about this

---

lmdb maxreaders

Should we apply back pressure once maxreaders is reached, easy to demonstrate - spawn 1000 concurrent threads
some of them will crash.

---

More aggregates:

all/any/not-any?
pg: bool_or bool_and

---

sets and map native filter eq

at the moment codec assumes it can sort keys so that buffer comparison can work. We should support maps (and later sets) where this is not possible.
