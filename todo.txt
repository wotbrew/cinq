Up/Del target

Currently syntax is quite straightforward but limited

(c/update [a b] (inc a)) ;; update all a's in b by incrementing them

The common case with updates will be to supply a single :from binding and a :where filter.

The first binding is always assumed to be the target relvar. This means you cannot say, start your query on another table (say 'd') .

(c/update [c d, a b :when (= a c)] (inc a)) ;; updates d, not b.

Options:

Meta
(c/update [c d, ^:target a b :when (= a c)] (inc a))
* +common case is still straightforward

Explicit relvar + rsn alias
(c/update b [c d, a b :when (= a c)] a (inc a))

Explicit alias, pass relvar to scan
(c/update [c d, a b :when (= a c)] a (inc a))

The delete and write actually work, is they target a relvar and rsn, maybe we make it explicit.
You could write transactions that modify, delete and insert several records in one loop, and then have a special case for simple functions
(c/modify
   [c d
    a b
    :when (= a c)]
   (if (even? a)
    ;; expand to (do (p/delete a:cinq/relvar a:cinq/rsn) (p/insert a:cinq/relvar (inc a)))
    (c/update a (inc a))
    ;; expand to (p/delete a:cinq/relvar a:cinq/rsn)
    (c/delete a)))

;; Function shorthand for simple cases
;; takes, relvar (target), predicate, expr
(c/update-where b [a] (even? a) (inc a)) ; (c/modify [a b :when (even? a)] (c/replace a (inc a)))
(c/delete-where b [a] (even? a)) ; (c/modify [a b :when (even? a)] (c/delete a))

;; this removes the need for relation proxy also, index entries will only supply rows under the val
(c/delete-all (b-idx 42)) ; (c/modify [_ (b-idx 42)] (c/delete _))
(c/update-all (b-idx 42) [a] (inc a)) ; (c/modify [a (b-idx 42)] (c/replace a (inc a)))

;;
(c/delete-where b [a] (even? a))

---

Implicit group

(q [f foo] (c/count))

if let/proj/where/join/scan-src/group-expr/order-expr expr expects grouping, add it if is not already present

---

Spilling

spilled merge join
(q [c customers
    :join! [o orders (= o:customer-id c:customer-id]]
    [c o])

spilled group by
(q [c customers
    :group! [firstname c:firstname]]
    [firstname (c/count)])

spilled sorts
(q [c customers
    :order! [c:firstname :desc]]
    c:id)

---

CinqDynamicBigRecord
- used if map is big
- actual hash table with lazy value loads

---

CinqDynamicList
- perhaps for large lists, lazy decoding would be helpful

---

CinqDynamicSet
(same as maps)

---

Record specialisation (structs)

(c/defrecord Customer [a b c d e f])

adds an actual defrecord, with a type hint.
record name is mapped to a layout (array of name, type)

layouts are stored in the symbol table
records are stored as layout value* buffers

(q [^Customer c customers])
 provides a layout hint to (scan customers) so that decoder can be specialised

if layout hint not matched adapt to the record using map->X ?
Or perhaps emitted/hinted type should be an interface, which can have an adapter for a lazy record

---

Hardening

- variable limit (LMDB)
- symbol limit (warn and degrade?)

---

Datalog

For recursive queries, I think datalog would be a better fit than the sql-style cinq q.

Datalog definition will rely on semantics suggestive of 'applying a relation as a function'. This invokes a generative binding mode

(require '[com.wotbrew.cinq.datalog :as dlog])

form (dlog/q ?selection ?projection)

standard relation application is generative, either a from (first application) or a join.

(dlog/q [(customers {?id :id})] ?id))
=>
(q [{?id :id} (generate customers {})] ?id)

a generate call on a returns a relation, It is implemented by relations and by functions.

(dlog/q
 [(customers {?id :id, ?firstname :firstname})
  (= ?id 42)]
 ?firstname)
=>
(q [{?id :id, ?firstname :firstname} (generate customers {})
    :join [_ (generate = ?id 42)]
  ?firstname)

Top level relation application will be turned into (generate) calls, but for common predicates - or predicates that are flagged as such
can get optimised away into :when clauses.

Normal relations will accept unary argument with a map of bindings for generate. Datalog rules will accept positional arguments.

(q [{?id :id} (generate customers {})
    :when (= ?id 42)])

;; dependent joins can be optimised into joins by rewriting generate calls
(dlog/q
 [(customers {?id :id})
  (orders {?id :customer-id, :order-id ?o})]
 [?id ?o]
 )
;; =>
(q [{?id :id} (generate customers {})
    :join [{?o :order-id} (generate orders {?id :customer-id})]
  [?id ?o])

rule:
(generate ?relvar {}) == ?relvar
(q [{?id :id} customers
    :join [_ (generate orders {?id :customer-id})]
  [?id o:order-id])

rule:
(generate ?relvar {?var ?att ...})
:join [x ?relvar (= ?var x:?att)]

RULEDEF

Rules look like lambdas, arity-overload used for multiple bodies. Can be defined inline or def'd like views.

(dlog/q
   [(dlog/r ancestor
     ([?a ?b]
      (parent {:child ?a, :parent ?b}))
     ([?a ?c]
      (parent {:child ?a, :parent ?b})
      (ancestor ?b ?c)))
    (ancestor ?a "bob")]
    ?a
   )

is transformed to a cte expression.

(c/cte [ancestor (c/q [p parent] [p:child p:parent])
        ancestor (c/q [[?a ?b] ancestor
                       :join [[?b ?c] ancestor (= ?b ?c)])])]
  (c/q [[?a ?c0] ancestor
        :when (= ?c0 "bob")]
    ?a))

PROBLEM: macros, or, not etc?

---

Destructure numeric key

(q [[a b] foo] [a b])

vector destructures should leave a special (nth-lookup) form in the scan so that we choose nth over get or dispatch at runtime based
on whether associative?

---

Views / Rules

(c/defview ancestor [parent]
  parent
  (c/q [[a b] ancestor
        [c d] ancestor
        :when (= b c)]
        [a b]))

defines macro-like the ra
[:union parent $join]

Problems:
 - hygiene, do we need to gensym and unquote?
 - Perhaps all unresolvable globally unqualified symbols get renamed to gensym?

usage

(cte [parent2 [["bob" "phil"]]
      ancestor (ancestor parent2)]
   ancestor)

[:cte [[parent ...], [ancestor [:cte [[ancestor [:union parent $join]]]]]]]

rules:

flatten unions into same bind
flatten cte into cte

---

Non materialized CTE views

---

Mutual recursion

Not supported at the moment. How to make that fast?

Non recursivity carries to dependents, if a query/view is only dependent on non recursive queries it itself is non recursive
this would be a planner / ana change

---

Interrupted checks

Currently, if the loop gets out of control (CTE?) you are screwed as there are no interrupt checks.
This will likely be even more important later when you are scanning 2TB LMDB relations.

Putting interrupt checks naively on the main loop does come with a fairly significant perf cost or around 5%.

If reads were buffered this cost may go down? Or if we go down the columnar route we can put the check at vector points

---

Nil as a record

Yay or nay, I expect a few things might be trickier if nils are allowed (we'd need to box to disambiguate for e.g lmdb delete return)

---

LMDB map size

map resize lock, guarantee exclusivity!
ReadWrite lock, write lock used when map needs resizing (and possibly DDL?),
read lock used for lmdb transactions (read/write) consider fairness barging.
for now all transactions completely lock the database!

---

Test db per index

This could mean a some multiple of the number of tables as opened dbis and extra per transaction overhead. Test though, might be fine.
If we have a problem consider prefixing keys with a single byte (0 == table), (1+) == index. Can have a max of 254 indexes per table.

---

Escape analysis

If columns do not escape a query, we do not need to copy them into heap memory.

We are already doing this a little bit to choose an unsafe top-level type for records.

- if captured by a 'build' such as group/join hash table, we still can keep refs to native mem
- if used in an expression (or intermediate) expression that might escape (due to side effect or return from query, or embedding in a return)
  we need to copy

---

immu / bitemp

- immutable relvar
  implies system_from, system_to cols
  update = cut
  delete = cut

  implies $now denormalized index for each var. use c/latest to return a new db/tx which wraps all relvars
  implies $now index for each index (-> db c/latest :foo :id (get 42))

- bitemporal relvar

  implies system_from, system_to, valid_from, valid_to cols
  update = cut
  delete = cut

  implies $now denormalized index for each var. use c/latest to return a new db/tx which wraps all relvars
  implies $now index for each index (-> db c/latest :foo :id (get 42))

---

javascript

- queries emit a lot of code, what to do about that?
- indexeddb rather than lmdb
- async transactions / reads
- conditional compilation and specialisation based on prim types would not be present

---

Index intersection

(c/merge (c/get idx0 42) (c/range idx2 < 42))

Add a rsn scan interface method to indexes to do some int-intersection

---

Composite indexes

- lists / vectors should be ok?
- what does index-key look like
- how does partial index query work
- how are these encoded correctly

---

Hash only indexes

- indexing maps and sets will give you hash lookups but no range queries (obviously)
- prefix coding a hash (find algo that preserves unorderedness of structures)
- should sorted maps / sets get special treatment? i.e these could get a their own typeid?

---

Constraints

Unique (and primary?)
Foreign key
Schema

Encoded as data, saved in variables table

(c/create-relvar db :foo schema) supply initial schema

(c/set-schema relvar schema)
;; throw if cannot enforce

(c/drop-schema relvar)
;; wild west

upserts, insert-ignore etc possible with primary/unique

---

auto-incr

if named attribute can be set to reflect the rsn automatically e.g (c/auto-increment relvar :customer-id)

---

extended destructuring

current limitations

- nesting/composites, e.g [[a b]], {[x, y] :z}
- lookup syms as bare symbols in bind e.g (c/q [a:b coll] a:b)

Currently bind does not follow clj destructuring, as maps do not have a seq path for example

---

'in transaction dvar'

if in a transaction already, root var reads should not implicitly open read/write transaction

e.g (def ^:dynamic *transactions* {})

if-some [ tx (*transactions (.-db relvar)) ] proxy transaction variable instead

this can also help with nested transactions, throwing a nice error if opening a write tx inside a read tx

---

scan chunking

there is a lot of virtual call overhead calling .apply per-tuple.

we could permit buffering some number of tuples before calling .apply

- filtering can set bits in a bitmap
- apply can capture the loop with the reduced checks
- apply can take an array and a len parameter

in the case of native, you can .apply to an lmdb cursor, and specialise the loop, including binary filters.

---

prim maps for integer joins

joins could get specialised hash multimap for long keys. common case if we have auto-incr, sql data.



